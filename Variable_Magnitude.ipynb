{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0a495d",
   "metadata": {},
   "source": [
    "## Variable magnitude\n",
    "\n",
    "### Does the magnitude of the variable matter?\n",
    "In Linear Regression models, the scale of variables used to estimate the output matters. Linear models are of the type y = w x + b, where the regression coefficient w represents the expected change in y for a one unit change in x (the predictor). Thus, the magnitude of w is partly determined by the magnitude of the units being used for x. If x is a distance variable, just changing the scale from kilometers to miles will cause a change in the magnitude of the coefficient.\n",
    "\n",
    "In addition, in situations where we estimate the outcome y by contemplating multiple predictors x1, x2, ...xn, predictors with greater numeric ranges dominate over those with smaller numeric ranges.\n",
    "\n",
    "Gradient descent converges faster when all the predictors (x1 to xn) are within a similar scale, therefore making feature scaling useful for Neural Networks as well as Logistic Regression.\n",
    "\n",
    "In Support Vector Machines, feature scaling can decrease the time to find the support vectors.\n",
    "\n",
    "Finally, methods using Euclidean distances or distances in general are also affected by the magnitude of the features, as Euclidean distance is sensitive to variations in the magnitude or scales of the predictors. Therefore feature scaling is required for methods that utilise distance calculations like k-nearest neighbours (KNN) and k-means clustering.\n",
    "\n",
    "For more details on the above, follow the links in the Bonus Lecture of this section.\n",
    "\n",
    "In summary:\n",
    "\n",
    "### Maginutde matters because:\n",
    "\n",
    "- The regression coefficient is directly influenced by the scale of the variable\n",
    "- Variables with bigger magnitudes / value range dominate over the ones with smaller magnitudes / value range\n",
    "- Gradient descent converges faster when features are on similar scales\n",
    "- Feature scaling helps decrease the time to find support vectors for SVMs\n",
    "- Euclidean distances are sensitive to feature magnitude.\n",
    "\n",
    "### The machine learning models affected by the magnitude of the feature are:\n",
    "- Linear and Logistic Regression\n",
    "- Neural Networks\n",
    "- Support Vector Machines\n",
    "- KNN\n",
    "- K-means clustering\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- Principal Component Analysis (PCA)\n",
    "\n",
    "### Machine learning models insensitive to feature magnitude are the ones based on Trees:\n",
    "- Classification and Regression Trees\n",
    "- Random Forests\n",
    "- Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "025290d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ba457",
   "metadata": {},
   "source": [
    "## Load data with numerical values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da92fafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass   Age     Fare\n",
       "0         0       3  22.0   7.2500\n",
       "1         1       1  38.0  71.2833\n",
       "2         1       3  26.0   7.9250\n",
       "3         1       1  35.0  53.1000\n",
       "4         0       3  35.0   8.0500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the numerical variables of the Titanic Dataset\n",
    "data = pd.read_csv('train.csv', usecols = ['Pclass', 'Age', 'Fare', 'Survived'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7343a240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Survived      Pclass         Age        Fare\n",
       "count  891.000000  891.000000  714.000000  891.000000\n",
       "mean     0.383838    2.308642   29.699118   32.204208\n",
       "std      0.486592    0.836071   14.526497   49.693429\n",
       "min      0.000000    1.000000    0.420000    0.000000\n",
       "25%      0.000000    2.000000   20.125000    7.910400\n",
       "50%      0.000000    3.000000   28.000000   14.454200\n",
       "75%      1.000000    3.000000   38.000000   31.000000\n",
       "max      1.000000    3.000000   80.000000  512.329200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc34e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass _range:  2\n",
      "Age _range:  79.58\n",
      "Fare _range:  512.3292\n"
     ]
    }
   ],
   "source": [
    "for col in ['Pclass', 'Age', 'Fare']:\n",
    "    print(col, '_range: ', data[col].max()-data[col].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5947e1",
   "metadata": {},
   "source": [
    "The magnitude of the values of the 3 different variables and their ranges of values are quite different. Therefore, feature scaling could benefit the performance of several machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b3e647c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((623, 3), (268, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# let's separate into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[['Pclass', 'Age', 'Fare']].fillna(0),\n",
    "    data.Survived,\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2c032",
   "metadata": {},
   "source": [
    "Briefly, the transformation is given by:\n",
    "\n",
    "X_std = (X - X.min() / (X.max - X.min())\n",
    "\n",
    "And to transform the scaled feature back to its initial format:\n",
    "\n",
    "X_scaled = X_std * (max - min) + min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f6c1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d32376eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [0.64365971 0.30131421 0.06335433]\n",
      "Standard Deviation:  [0.41999093 0.21983527 0.09411705]\n",
      "Minimum value:  [0. 0. 0.]\n",
      "Maximum value:  [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#let's have a look at the scaled training dataset\n",
    "print('Mean: ', X_train_scaled.mean(axis=0))\n",
    "print('Standard Deviation: ', X_train_scaled.std(axis=0))\n",
    "print('Minimum value: ', X_train_scaled.min(axis=0))\n",
    "print('Maximum value: ', X_train_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b23cf",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cc6ef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.7134823539619531\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.7080952380952381\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# model build on unscaled variables\n",
    "\n",
    "logit = LogisticRegression(random_state=44, C=1000) # c big to avoid regularization\n",
    "logit.fit(X_train, y_train)\n",
    "print('Train set')\n",
    "pred = logit.predict_proba(X_train)\n",
    "print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = logit.predict_proba(X_test)\n",
    "print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c22ec415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.92585764, -0.01822689,  0.00233577]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a89247b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.7134931997136721\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.7080952380952381\n"
     ]
    }
   ],
   "source": [
    "# model built on scaled variables\n",
    "logit = LogisticRegression(random_state=44, C=1000) # c big to avoid regularization\n",
    "logit.fit(X_train_scaled, y_train)\n",
    "print('Train set')\n",
    "pred = logit.predict_proba(X_train_scaled)\n",
    "print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = logit.predict_proba(X_test_scaled)\n",
    "print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c82dc907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.85170244, -1.45782986,  1.19540159]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c514d",
   "metadata": {},
   "source": [
    "We observe that the performance of logistic regression did not change when using the datasets with the features scaled (compare roc-auc values for train and test set for models with and without feature scaling).\n",
    "\n",
    "However, when looking at the coefficients we do see a big difference in the values. This is because the magnitude of the variable was affecting the coefficients. After scaling, all 3 variables have the relative same effect (coefficient) for survival, whereas before scaling, we would be inclined to think that PClass was driving the Survival outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48fb173",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d239171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "SVM roc-auc: 0.6641558751437062\n",
      "Test set\n",
      "SVM roc-auc: 0.6845833333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# model build on data with plenty of categories in Cabin variable\n",
    "\n",
    "SVM_model = SVC(random_state=44, probability=True)\n",
    "SVM_model.fit(X_train, y_train)\n",
    "print('Train set')\n",
    "pred = SVM_model.predict_proba(X_train)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = SVM_model.predict_proba(X_test)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aac16361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "SVM roc-auc: 0.7007928244506626\n",
      "Test set\n",
      "SVM roc-auc: 0.6742857142857143\n"
     ]
    }
   ],
   "source": [
    "SVM_model = SVC(random_state=44, probability=True)\n",
    "SVM_model.fit(X_train_scaled, y_train)\n",
    "print('Train set')\n",
    "pred = SVM_model.predict_proba(X_train_scaled)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = SVM_model.predict_proba(X_test_scaled)\n",
    "print('SVM roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfad490",
   "metadata": {},
   "source": [
    "Feature scaling improved the performance of the support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e0bf1",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34aa5d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Neural Network roc-auc: 0.6012288236697686\n",
      "Test set\n",
      "Neural Network roc-auc: 0.565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# model built on unscaled features\n",
    "\n",
    "NN_model = MLPClassifier(random_state=44, solver='sgd')\n",
    "NN_model.fit(X_train, y_train)\n",
    "print('Train set')\n",
    "pred = NN_model.predict_proba(X_train)\n",
    "print('Neural Network roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = NN_model.predict_proba(X_test)\n",
    "print('Neural Network roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a340703d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Neural Network roc-auc: 0.7165300101950066\n",
      "Test set\n",
      "Neural Network roc-auc: 0.7124404761904761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEVI\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model built on scaled features\n",
    "\n",
    "NN_model = MLPClassifier(random_state=44, solver='sgd')\n",
    "NN_model.fit(X_train_scaled, y_train)\n",
    "print('Train set')\n",
    "pred = NN_model.predict_proba(X_train_scaled)\n",
    "print('Neural Network roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = NN_model.predict_proba(X_test_scaled)\n",
    "print('Neural Network roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cbcd4c",
   "metadata": {},
   "source": [
    "We observe that scaling the features improved the performance of the neural network both for the training and the testing set (compare roc-auc values: training 0.60 vs 0.71; testing: 0.5 vs 0.71). The roc-auc increases in both training and testing sets when the model is trained on a dataset with scaled features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c372f9",
   "metadata": {},
   "source": [
    "## K- Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17dd9cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "KNN roc-auc: 0.8694225721784778\n",
      "Test set\n",
      "KNN roc-auc: 0.6253571428571428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#model built on unscaled features\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "KNN.fit(X_train, y_train)\n",
    "print('Train set')\n",
    "pred = KNN.predict_proba(X_train)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = KNN.predict_proba(X_test)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26c6b05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "KNN roc-auc: 0.8880555736318084\n",
      "Test set\n",
      "KNN roc-auc: 0.7017559523809525\n"
     ]
    }
   ],
   "source": [
    "# model built on scaled\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "KNN.fit(X_train_scaled, y_train)\n",
    "print('Train set')\n",
    "pred = KNN.predict_proba(X_train_scaled)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = KNN.predict_proba(X_test_scaled)\n",
    "print('KNN roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7fec40",
   "metadata": {},
   "source": [
    "We observe for KNN as well that feature scaling improved the performance of the model. The model built on unscaled features shows a better generalisation, with a higher roc-auc for the testing set (0.86 vs 0.88 for model built on unscaled features).\n",
    "\n",
    "Both KNN methods are over-fitting to the train set. Thus, we would need to change the parameters of the model or use less features to try and decrease over-fitting, which exceeds the purpose of this demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad504d",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f768bcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9914589705212469\n",
      "Test set\n",
      "Random Forests roc-auc: 0.760327380952381\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# model built on unscaled features\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=700, random_state=39)\n",
    "rf.fit(X_train, y_train)\n",
    "print('Train set')\n",
    "pred = rf.predict_proba(X_train)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = rf.predict_proba(X_test)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68d4929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.991491507776404\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7610119047619048\n"
     ]
    }
   ],
   "source": [
    "# model built in scaled features\n",
    "rf = RandomForestClassifier(n_estimators=700, random_state=39)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "print('Train set')\n",
    "pred = rf.predict_proba(X_train_scaled)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = rf.predict_proba(X_test_scaled)\n",
    "print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4952a916",
   "metadata": {},
   "source": [
    "As expected, Random Forests shows no change in performance regardless of whether it is trained on a dataset with scaled or unscaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78431154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "AdaBoost roc-auc: 0.8477364916162339\n",
      "Test set\n",
      "AdaBoost roc-auc: 0.7733630952380953\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(n_estimators=200, random_state=44)\n",
    "ada.fit(X_train, y_train)\n",
    "print('Train set')\n",
    "pred = ada.predict_proba(X_train)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = ada.predict_proba(X_test)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a27c6884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "AdaBoost roc-auc: 0.8477364916162339\n",
      "Test set\n",
      "AdaBoost roc-auc: 0.7733630952380953\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=200, random_state=44)\n",
    "ada.fit(X_train_scaled, y_train)\n",
    "print('Train set')\n",
    "pred = ada.predict_proba(X_train_scaled)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "print('Test set')\n",
    "pred = ada.predict_proba(X_test_scaled)\n",
    "print('AdaBoost roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f9bd5",
   "metadata": {},
   "source": [
    "As expected, AdaBoost shows no change in performance regardless of whether it is trained on a dataset with scaled or unscaled features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
